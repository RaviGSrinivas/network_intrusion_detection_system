{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.decomposition import RandomizedPCA \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load UNSW_NB15 into a Pandas dataframe\n",
    "df = pd.read_csv('UNSW_NB15_training_set.csv', encoding='utf-8-sig')\n",
    "df_five = df[['sttl','ct_dst_sport_ltm', 'ct_src_dport_ltm', 'swin', 'dwin', 'label' ]] # \n",
    "\n",
    "\n",
    "# Remove the four duplicate rows with invalid value for is_ftp_login\n",
    "df = df[df.is_ftp_login != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation:\n",
    "\n",
    "* Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate record deleted successfully: 82328 observations remaining\n"
     ]
    }
   ],
   "source": [
    "# Lets remove attributes that are not useful to us during this first analysis pass\n",
    "non_useful_features_list = ['id', 'attack_cat']\n",
    "# id: n internal variable to just ref an obseration. deemed not usefl\n",
    "# attack_cat: first try and just predict the label. \n",
    "#             It will obviously 1:1 correlate with label\n",
    "#             We can circle back and swap it out with label \n",
    "#             to see if we get any better accuracy on an \n",
    "#             on an attack type level\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        df.drop(feature, axis=1, inplace=True)  # Lets drop id as it is an internal variable to just ref an obseratio\n",
    "        \n",
    "# Overwrite the existing dataframe with the new dataframe that does not contain the \n",
    "# four unwanted records and confirm we have 4 less records (shold have 82328 observations)\n",
    "if \"is_ftp_login\" in df:\n",
    "    df = df[df.is_ftp_login != 2]\n",
    "    if len(df) == 82328:\n",
    "        print \"duplicate record deleted successfully: \" + str(len(df)) + \" observations remaining\" \n",
    "        \n",
    "# Check to see if non useful features still exist in dataframe, if so, we did something wrong\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        print \"[\" + feature + \"]\" + \"still found, check removal code. (Should not see this)\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "\n",
    "# Surrounding code in try/except on case where there are no object type features to one-hot encode\n",
    "try:\n",
    "    tmp_df = df.describe(include=['O'])  # creates a temporary df with just categorical features that are of object type\n",
    "    categorical_object_col_name_list = tmp_df.columns.values.tolist()\n",
    "    for col_name in categorical_object_col_name_list:\n",
    "        #print col_name\n",
    "        tmp_df = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "        df = pd.concat((df,tmp_df), axis=1)\n",
    "        df.drop(col_name, axis=1, inplace=True)  # go ahead and drop original feature as it has now been one-hot encoded\n",
    "except ValueError as e:\n",
    "    print \"Value error({0}): \".format(e)  # Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 190 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(28)\n",
      "memory usage: 120.0 MB\n",
      "{0: 37000, 1: 45328}\n",
      "Percent normal(0) is 45%\n",
      "Percent attack(1) is 55%\n"
     ]
    }
   ],
   "source": [
    "dfcopy = df.copy(deep=True) # preserve original dataframe that has our dependent variable\n",
    "dfcopy.info()\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'label' in dfcopy:\n",
    "    y = dfcopy['label'].values # get the labels we want\n",
    "    del dfcopy['label'] # get rid of the class label\n",
    "    X = dfcopy.values # use everything else to predict!\n",
    "dfcopy.info() # should have 190 entries\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# determine if we have roughly the same percentage of  normal vs abnormal observations\n",
    "# ref: http://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray-in-python\n",
    "# ref: http://www.gossamer-threads.com/lists/python/python/809232 (print percentage)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "uniqueClassCounts = dict(zip(unique, counts))    \n",
    "totalObservations = len(y)\n",
    "print uniqueClassCounts\n",
    "print \"Percent normal(0) is {0:.0%}\".format(float(uniqueClassCounts[0])/totalObservations)\n",
    "print \"Percent attack(1) is {0:.0%}\".format(float(uniqueClassCounts[1])/totalObservations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "(a) http://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation\n",
    "(b) http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "(c) http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
    "\n",
    "\n",
    "Per ref (a) startification in general seeks to ensure each fold is representative of all strata in the data. The label feature has two classes (categories) as seen above normal(0) and attack(1) with percentages of 45% and 55%, respectively based on distribution of the classes with respect to the complete dataset. If it was 50%/50% one could argue a random sampling should create evenly distributed folds and with large enough sample size, should be sufficient. The main argument for stratification is to address the biasing effects of classification algorithms based on over/under representation of classes in any one particular sample or folds for classification algorithms that do not inherently have inherent balancing techniques either by selection or weighting.  According to ref (a), however, it does lead to a loss of diversity (unwanted loss of variance). Basically, the more unbalanced then the classes are, the more biased a classification algorithm in general would be to the observations tending to the more popular class. Stratification seeks to correct for that artificially to address algorithms that cleverly try and predict the class witht the highest weighting based on class distribution.\n",
    "\n",
    "Since the values in the labels are not exactly equal, a stratified Kfold validation is recommended so that each set or fold contains approximately the same percentage of samples of each target class as the complete set.\n",
    "Note: Will do both for comparision sake to see if there is any difference\n",
    "in results and to get a feel for the methodology.\n",
    "\n",
    "Note: per ref (a) To combat imbalanced training data, one technique woudl be to try and colect more data with a larger dataset.  However. since we did not generate this data ourselves, this would not be an option.\n",
    "\n",
    "Another thing that we can do to to measure performance is to selecting the right set of performance metrics beyond the arguably defalt one which is accuracy.  \n",
    "\n",
    "With our particular dataset there are a couple of different view points on which performance metrics a model should be tuned for for this particular dataset:\n",
    "1) False negatives are probably worse than false positives. If a malicious packet gets through undetected, it could potentially do bad things without being noticed, whereas a false positive could be rescreened or reviewed to clear the packet. Note: however, if the system has protection in depth, perhaps down stream systems may catch the packet or some effect of the malicous abnormal packet (e.g. host based intrusion detection system, so it may not be that bad).\n",
    "Recall may be a good metric to gauge model performance based on highlighting false negatives. Recall is defined as the number of True Postivies divided by the number of True Positives and the number of False Negatives, also known as Sensitivity or the True Postive Rate. \n",
    "\n",
    "2) However, if there are too may false positives, where the system over classifies packets as malcious when they really aren't then it could be interfering with normal operations and impact business operations. On the assumption that it takes some signficant amount of time to clear a tagged (false) positive packet. If a tool identifies too many false positives, people would lose trust with the system. Based on this view, precision which is defined as the number of True Positives divided by the number of True Positives and False positives would be a good metric to use to gauge performance of models used for predicting abnormal packets.\n",
    "\n",
    "3) Based on these view points, a metric which combines both would be the F1 Score which is 2*((precision*recall)/(precision+recall)) also know as the F Score or F Measure which tries to convey the balance between precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82328\n",
      "sklearn.cross_validation.KFold(n=82328, n_folds=10, shuffle=False, random_state=None)\n",
      "Average Accuracy across 10 fold cross validation iterations = 0.606487290653 training time = 10.2802627327\n",
      "Average Accuracies across statified 10 fold cross validation iterations = 75%  training time = 10.3222235803\n",
      "Average Precision across statified 10 fold cross validation iterations = 85% training time = 10.3222235803\n",
      "Average Recall(Sensitivity) across statified 10 fold cross validation iterations = 66% training time = 10.3222235803\n",
      "Average Accuracy across statified 10 fold cross validation iterations = 75% training time = 10.3222235803\n",
      "Average F measure(F1) across statified 10 fold cross validation iterations = 73% training time = 10.3222235803\n",
      "(0.67579947052149436, 0.86077042667127313, 0.75715162668194602)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "#ref : http://stackoverflow.com/questions/2866380/how-can-i-time-a-code-segment-for-testing-performance-with-pythons-timeit\n",
    "import time\n",
    "#ref: http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.KFold.html\n",
    "numObservations = len(dfcopy)\n",
    "print numObservations\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "y = dfcopy['label'].values\n",
    "numObservations = len(dfcopy)\n",
    "num_folds = 10\n",
    "kf = KFold(numObservations, n_folds=num_folds)\n",
    "skf = StratifiedKFold(y, num_folds)\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "# here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "print(kf)\n",
    "t0 = time.clock()\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test)\n",
    "t1 = time.clock()\n",
    "total = t1 - t0\n",
    "\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=kf) # this also can help with parallelism\n",
    "#print(accuracies)\n",
    "print \"Average Accuracy across \" + str(num_folds) + \" fold cross validation iterations = \" + str(np.average(accuracies)) + \" training time = \" + str(total)    \n",
    "\n",
    "t0 = time.clock()\n",
    "for train_index, test_index in skf:\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test)\n",
    "t1 = time.clock()\n",
    "total = t1 - t0\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=skf) # this also can help with parallelism\n",
    "precision = cross_val_score(lr_clf, X, y=y, cv=skf, scoring='precision')\n",
    "recall = cross_val_score(lr_clf, X, y=y, cv=skf, scoring='recall')\n",
    "accuracy = cross_val_score(lr_clf, X, y=y, cv=skf, scoring='accuracy')\n",
    "f1 = cross_val_score(lr_clf, X, y=y, cv=skf, scoring='f1')# just testing to see if it is same as default\n",
    "#print(accuracies)\n",
    "#print \"Percent normal(0) is {0:.0%}\".format(float(classCounts[0])/totalObservations)\n",
    "print \"Average Accuracies across stratified \" + str(num_folds) + \" fold cross validation iterations = {0:.0%} \".format(np.average(accuracies)) + \" training time = \" + str(total)    \n",
    "print \"Average Precision across stratified \" + str(num_folds) + \" fold cross validation iterations = {0:.0%}\".format(np.average(precision)) + \" training time = \" + str(total)    \n",
    "print \"Average Recall(Sensitivity) across stratified \" + str(num_folds) + \" fold cross validation iterations = {0:.0%}\".format(np.average(recall)) + \" training time = \" + str(total)    \n",
    "print \"Average Accuracy across stratified \" + str(num_folds) + \" fold cross validation iterations = {0:.0%}\".format(np.average(accuracy)) + \" training time = \" + str(total)    \n",
    "print \"Average F measure(F1) across stratified \" + str(num_folds) + \" fold cross validation iterations = {0:.0%}\".format(np.average(f1)) + \" training time = \" + str(total)    \n",
    "\n",
    "# http://stackoverflow.com/questions/23339523/sklearn-cross-validation-with-multiple-scores\n",
    "from sklearn import metrics\n",
    "def mean_scores(X, y, clf, skf):\n",
    "\n",
    "    cm = np.zeros(len(np.unique(y)) ** 2)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        clf.fit(X[train], y[train])\n",
    "        y_pred = clf.predict(X[test])\n",
    "        cm += metrics.confusion_matrix(y[test], y_pred).flatten()\n",
    "\n",
    "    return compute_measures(*cm / skf.n_folds)\n",
    "\n",
    "def compute_measures(tp, fp, fn, tn):\n",
    "     \"\"\"Computes effectiveness measures given a confusion matrix.\"\"\"\n",
    "     specificity = tn / (tn + fp)\n",
    "     sensitivity = tp / (tp + fn)\n",
    "     fmeasure = 2 * (specificity * sensitivity) / (specificity + sensitivity)\n",
    "     return sensitivity, specificity, fmeasure\n",
    "    \n",
    "print mean_scores(X, y, lr_clf, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stratified 10 fold cross validation appears to have a higher accuracy score of 75% vs 61%. Not much difference in computational expense between the two cross validation methods.\n",
    "    # https://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/\n",
    "   \n",
    "   Sensitivy/recall measures how good the model is at detecting the positives, which somewhat important.\n",
    "However, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ref: http://matthewrocklin.com/blog/work/2016/07/12/dask-learn-part-1\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=10000,\n",
    "                           n_features=500,\n",
    "                           n_classes=2,\n",
    "                           n_redundant=250,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca),\n",
    "                       ('logistic', logistic)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "grid = dict(pca__n_components=[50, 100, 250],\n",
    "            logistic__C=[1e-4, 1.0, 1e4],\n",
    "            logistic__penalty=['l1', 'l2'])\n",
    "\n",
    "scores = ['precision', 'recall', 'f1', 'accuracy']\n",
    "\n",
    "#grid = dict(pca__n_components=[50],\n",
    "#            logistic__C=[1e-4],\n",
    "#            logistic__penalty=['l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.8 s\n",
      "precision 0.89184751014\n",
      "Wall time: 35.7 s\n",
      "recall 0.8948\n",
      "Wall time: 34.8 s\n",
      "f1 0.893126414374\n",
      "Wall time: 35.2 s\n",
      "accuracy 0.8928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from __future__ import print_function\n",
    "\n",
    "for score in scores:\n",
    "    estimator = GridSearchCV(pipe, grid, n_jobs=-1, scoring='%s' % score)\n",
    "    %time estimator.fit(X, y)\n",
    "    print(score + \" \" + str(estimator.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic__C': 0.0001, 'logistic__penalty': 'l2', 'pca__n_components': 50}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.016) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.959 (+/-0.029) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.982 (+/-0.026) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.982 (+/-0.025) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.982 (+/-0.025) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 1}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 10}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 100}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.019) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.957 (+/-0.029) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.981 (+/-0.028) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.981 (+/-0.026) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.981 (+/-0.026) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 1}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 10}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 100}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ref: http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    # http://stackoverflow.com/questions/1614236/in-python-how-to-i-convert-all-items-in-a-list-to-floats\n",
    "    print(classification_report(y_true, y_pred, target_names=[str(i) for i in digits.target_names.tolist()]))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Modeling and Evaluation:\n",
    "* Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment:\n",
    "* How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
